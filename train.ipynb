{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v2_0,ACCESS_BARRA_v2_1\n",
    "import torch\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    my_log_file=open(\"./model/save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     pre_train_path=\"./model/save/temp01/\"+0+\".pth\"\n",
    "\n",
    "    \n",
    "    \n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 2)\n",
    "    end_date=date(2011,12,25)\n",
    "#     end_date=date(2012,12,25) #if 929 is true we should substract 1 day    \n",
    "    sys = platform.system()\n",
    "    args.file_ACCESS_dir=\"../data/\"\n",
    "    args.file_BARRA_dir=\"../data/barra_aus/\"\n",
    "#     if sys == \"Windows\":\n",
    "#         init_date=date(1970, 1, 1)\n",
    "#         start_date=date(1990, 1, 2)\n",
    "#         end_date=date(1990,12,15) #if 929 is true we should substract 1 day   \n",
    "#         args.cpu=True\n",
    "# #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "# #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "#         args.file_DEM_dir=\"../DEM/\"\n",
    "#     else:\n",
    "#         args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "#         args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "#         # training_name=\"temp01\"\n",
    "#         args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "    pre_train_path=\"./model/save/\"+args.train_name+\"/last_\"+str(args.channels)+\".pth\"\n",
    "    leading_time=217\n",
    "    args.leading_time_we_use=1\n",
    "    args.ensemble=1\n",
    "\n",
    "\n",
    "    print(access_rgb_mean)\n",
    "\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu onlyï¼Ÿ | %5d\"%args.cpu)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "    #     transforms.Resize(IMG_SIZE),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "#     data_set=ACCESS_BARRA_v2_0(start_date,end_date,transform=train_transforms,args=args)\n",
    "    data_set=ACCESS_BARRA_v2_1(start_date,end_date,transform=train_transforms,args=args)\n",
    "\n",
    "    train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  total | %5d\"%len(data_set))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  train | %5d\"%len(train_data))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  val   | %5d\"%len(val_data))\n",
    "\n",
    "    ###################################################################################set a the dataLoader\n",
    "    train_dataloders =DataLoader(train_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                num_workers=args.n_threads)\n",
    "    val_dataloders =DataLoader(val_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                              num_workers=args.n_threads)\n",
    "    ##\n",
    "    def prepare( l, volatile=False):\n",
    "        def _prepare(tensor):\n",
    "            if args.precision == 'half': tensor = tensor.half()\n",
    "            if args.precision == 'single': tensor = tensor.float()\n",
    "            return tensor.to(device)\n",
    "\n",
    "        return [_prepare(_l) for _l in l]\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint = utility.checkpoint(args)\n",
    "    net = model.Model(args, checkpoint)\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=True)\n",
    "    my_net=my_model.Modify_RCAN(net,args,checkpoint)\n",
    "\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=args.cpu)\n",
    "    \n",
    "    args.lr=0.00001\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "    # torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n",
    "    \n",
    "#     if args.resume==1:\n",
    "#         print(\"continue last train\")\n",
    "#         model_checkpoint = torch.load(pre_train_path,map_location=device)\n",
    "#     else:\n",
    "#         print(\"restart train\")\n",
    "#         model_checkpoint = torch.load(\"./model/save/\"+args.train_name+\"/first_\"+str(args.channels)+\".pth\",map_location=device)\n",
    "\n",
    "#     my_net.load_state_dict(model_checkpoint['model'])\n",
    "#     optimizer_my.load_state_dict(model_checkpoint['optimizer'])\n",
    "#     epoch = model_checkpoint['epoch']\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        net = nn.DataParallel(net)\n",
    "    else:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "#     my_net = torch.nn.DataParallel(my_net)\n",
    "    net.to(device)\n",
    "    \n",
    "    ##########################################################################training\n",
    "\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        scheduler.step()\n",
    "        net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,hr,_,_) in enumerate(train_dataloders):\n",
    "            write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            start=time.time()\n",
    "            pr,hr= prepare([pr,hr])\n",
    "\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = net(pr,0)\n",
    "                running_loss =criterion(sr, hr)\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "                \n",
    "            loss+=running_loss #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "            write_log(\"Train done,train time cost %f s,loss: %f\"%(start-time.time(),running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for idx_img, (lr,hr,_,_) in enumerate(val_dataloders):\n",
    "                lr,hr = prepare([lr, hr])\n",
    "                sr = net(lr,0)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "\n",
    "        if running_loss<max_error:\n",
    "            max_error=running_loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\".pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5114674452354135\n",
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  temp01\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     0\n",
      "  ------------------------------\n",
      "  batch_size     |     2\n",
      "  ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 256, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 101, in main\n",
      "    print(\"  using cpu only\\uff1f | %5d\"%args.cpu)\n",
      "  File \"E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\uff1f' in position 16: character maps to <undefined>\n"
     ]
    }
   ],
   "source": [
    "!python train.py  --n_threads 0  --batch_size 2 --n_resgroups 10 --n_resblocks 20 --patch_size 192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
