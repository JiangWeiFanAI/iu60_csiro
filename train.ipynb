{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "parser.add_argument('--prprpr', action='store_true',\n",
    "                    help='Enables debug mode')\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "                    help='Enables debug mode')\n",
    "parser.add_argument('--template', default='.',\n",
    "                    help='You can set various templates in option.py')\n",
    "\n",
    "# Hardware specifications\n",
    "parser.add_argument('--n_threads', type=int, default=0,\n",
    "                    help='number of threads for data loading')\n",
    "parser.add_argument('--cpu', action='store_true',\n",
    "                    help='use cpu only')\n",
    "parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                    help='number of GPUs')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "\n",
    "# Data specifications\n",
    "\n",
    "parser.add_argument('--pr', type=bool, \n",
    "                default=True,\n",
    "                help='add-on pr?')\n",
    "\n",
    "parser.add_argument('--dem', action='store_true',\n",
    "                help='add-on dem?') \n",
    "parser.add_argument('--psl', action='store_true',\n",
    "                help='add-on psl?') \n",
    "parser.add_argument('--zg', action='store_true',\n",
    "                help='add-on zg?') \n",
    "parser.add_argument('--tasmax', action='store_true',\n",
    "                help='add-on tasmax?') \n",
    "parser.add_argument('--tasmin', action='store_true',\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "# parser.add_argument('--pr', type=bool, \n",
    "#                 default=True,\n",
    "#                 help='add-on pr?')\n",
    "\n",
    "# parser.add_argument('--dem', type=bool, \n",
    "#                 default=False,\n",
    "#                 help='add-on dem?') \n",
    "# parser.add_argument('--psl', type=bool, \n",
    "#                 default=False,\n",
    "#                 help='add-on psl?') \n",
    "# parser.add_argument('--zg', type=bool, \n",
    "#                 default=False,\n",
    "#                 help='add-on zg?') \n",
    "# parser.add_argument('--tasmax', type=bool, \n",
    "#                 default=False,\n",
    "#                 help='add-on tasmax?') \n",
    "# parser.add_argument('--tasmin', type=bool, \n",
    "#                 default=False,\n",
    "#                 help='add-on tasmin?')\n",
    "\n",
    "parser.add_argument('--leading_time_we_use', type=int, \n",
    "                default=7,\n",
    "                help='add-on tasmin?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--ensemble', type=int, \n",
    "                default=2,\n",
    "                help='total ensambles is 11') \n",
    "\n",
    "\n",
    "parser.add_argument('--channels', type=float, \n",
    "                    default=0,\n",
    "                    help='channel of data_input must') \n",
    "#[111.85, 155.875, -44.35, -9.975]\n",
    "parser.add_argument('--domain', type=list, \n",
    "                    default=[112.9, 154.25, -43.7425, -9.0],\n",
    "                    help='dataset directory')    \n",
    "\n",
    "\n",
    "parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                    default=\"../data/\",\n",
    "\n",
    "                    help='dataset directory')\n",
    "parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                    default=\"../data/barra_aus/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--file_DEM_dir', type=str, \n",
    "                    default=\"../DEM/\",\n",
    "                    help='dataset directory')\n",
    "\n",
    "parser.add_argument('--nine2nine', type=bool, \n",
    "                    default=True,\n",
    "                    help='whether rainfall acculate from 9am to 9am')\n",
    "parser.add_argument('--date_minus_one', type=int, \n",
    "                    default=1,\n",
    "                    help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "\n",
    "\n",
    "parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                    help='demo image directory')\n",
    "#     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "#                         help='train dataset name')\n",
    "#     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "#                         help='test dataset name')\n",
    "parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                    help='use noisy benchmark sets')\n",
    "parser.add_argument('--n_train', type=int, default=800,\n",
    "                    help='number of training set')\n",
    "parser.add_argument('--n_val', type=int, default=10,\n",
    "                    help='number of validation set')\n",
    "parser.add_argument('--offset_val', type=int, default=800,\n",
    "                    help='validation index offest')\n",
    "parser.add_argument('--ext', type=str, default='sep',\n",
    "                    help='dataset file extension')\n",
    "parser.add_argument('--scale', default='4',\n",
    "                    help='super resolution scale')\n",
    "parser.add_argument('--patch_size', type=int, default=96,\n",
    "                    help='output patch size')\n",
    "#??????????????????????????????????????????????????\n",
    "parser.add_argument('--rgb_range', type=int, default=255,\n",
    "                    help='maximum value of RGB')\n",
    "parser.add_argument('--n_colors', type=int, default=1,\n",
    "                    help='number of color channels to use')\n",
    "parser.add_argument('--noise', type=str, default='.',\n",
    "                    help='Gaussian noise std.')\n",
    "parser.add_argument('--chop', action='store_true',\n",
    "                    help='enable memory-efficient forward')\n",
    "\n",
    "# Model specifications\n",
    "parser.add_argument('--model', default='RCAN',\n",
    "                    help='model name')\n",
    "\n",
    "parser.add_argument('--act', type=str, default='relu',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--continue_train', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "\n",
    "parser.add_argument('--pre_train', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--extend', type=str, default='.',\n",
    "                    help='pre-trained model directory')\n",
    "parser.add_argument('--n_resblocks', type=int, default=16,\n",
    "                    help='number of residual blocks')\n",
    "parser.add_argument('--n_feats', type=int, default=64,\n",
    "                    help='number of feature maps')\n",
    "parser.add_argument('--res_scale', type=float, default=1,\n",
    "                    help='residual scaling')\n",
    "parser.add_argument('--shift_mean', default=True,\n",
    "                    help='subtract pixel mean from the input')\n",
    "parser.add_argument('--precision', type=str, default='single',\n",
    "                    choices=('single', 'half','double'),\n",
    "                    help='FP precision for test (single | half)')\n",
    "\n",
    "# Training specifications\n",
    "\n",
    "parser.add_argument('--train_name', type=str, default='temp01',\n",
    "                    help='the trainning name of the set')\n",
    "parser.add_argument('--reset', action='store_true',\n",
    "                    help='reset the training')\n",
    "parser.add_argument('--test_every', type=int, default=1000,\n",
    "                    help='do test per every N batches')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size', type=int, default=16,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--split_batch', type=int, default=1,\n",
    "                    help='split the batch into smaller chunks')\n",
    "parser.add_argument('--self_ensemble', action='store_true',\n",
    "                    help='use self-ensemble method for test')\n",
    "parser.add_argument('--test_only', action='store_true',\n",
    "                    help='set this option to test the model')\n",
    "parser.add_argument('--gan_k', type=int, default=1,\n",
    "                    help='k value for adversarial loss')\n",
    "\n",
    "# Optimization specifications\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                    help='learning rate decay per N epochs')\n",
    "parser.add_argument('--decay_type', type=str, default='step',\n",
    "                    help='learning rate decay type')\n",
    "parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                    help='learning rate decay factor for step decay')\n",
    "parser.add_argument('--optimizer', default='ADAM',\n",
    "                    choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                    help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='ADAM beta1')\n",
    "parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                    help='ADAM beta2')\n",
    "parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                    help='ADAM epsilon for numerical stability')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "\n",
    "# Loss specifications\n",
    "parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                    help='loss function configuration')\n",
    "parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                    help='skipping batch that has large error')\n",
    "\n",
    "# Log specifications\n",
    "parser.add_argument('--save', type=str, default='My_RCAN',\n",
    "                    help='file name to save')\n",
    "parser.add_argument('--load', type=str, default='.',\n",
    "                    help='file name to load')\n",
    "parser.add_argument('--resume', type=int, default=0,\n",
    "                    help='resume from specific checkpoint')\n",
    "parser.add_argument('--print_model', action='store_true',\n",
    "                    help='print model')\n",
    "parser.add_argument('--save_models', action='store_true',\n",
    "                    help='save all intermediate models')\n",
    "parser.add_argument('--print_every', type=int, default=100,\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save_results', action='store_true',\n",
    "                    help='save output results')\n",
    "\n",
    "# New options\n",
    "parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                    help='number of residual groups')\n",
    "parser.add_argument('--reduction', type=int, default=16,\n",
    "                    help='number of feature maps reduction')\n",
    "parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                    help='dataset directory for testing')\n",
    "parser.add_argument('--testset', type=str, default='Set5',\n",
    "                    help='dataset name for testing')\n",
    "parser.add_argument('--degradation', type=str, default='BI',\n",
    "                    help='degradation model: BI, BD')\n",
    "\n",
    "# import platform \n",
    "# sys = platform.system()\n",
    "# if sys == \"Windows\":\n",
    "#     args = parser.parse_args(args=[])\n",
    "# else:\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "#     template.set_template(args)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "if args.epochs == 0:\n",
    "    args.epochs = 1e8\n",
    "\n",
    "for arg in vars(args):\n",
    "    if vars(args)[arg] == 'True':\n",
    "        vars(args)[arg] = True\n",
    "    elif vars(args)[arg] == 'False':\n",
    "        vars(args)[arg] = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--prprpr] [--debug] [--template TEMPLATE]\n",
      "                             [--n_threads N_THREADS] [--cpu] [--n_GPUs N_GPUS]\n",
      "                             [--seed SEED] [--pr PR] [--dem] [--psl] [--zg]\n",
      "                             [--tasmax] [--tasmin]\n",
      "                             [--leading_time_we_use LEADING_TIME_WE_USE]\n",
      "                             [--ensemble ENSEMBLE] [--channels CHANNELS]\n",
      "                             [--domain DOMAIN]\n",
      "                             [--file_ACCESS_dir FILE_ACCESS_DIR]\n",
      "                             [--file_BARRA_dir FILE_BARRA_DIR]\n",
      "                             [--file_DEM_dir FILE_DEM_DIR]\n",
      "                             [--nine2nine NINE2NINE]\n",
      "                             [--date_minus_one DATE_MINUS_ONE]\n",
      "                             [--dir_demo DIR_DEMO] [--benchmark_noise]\n",
      "                             [--n_train N_TRAIN] [--n_val N_VAL]\n",
      "                             [--offset_val OFFSET_VAL] [--ext EXT]\n",
      "                             [--scale SCALE] [--patch_size PATCH_SIZE]\n",
      "                             [--rgb_range RGB_RANGE] [--n_colors N_COLORS]\n",
      "                             [--noise NOISE] [--chop] [--model MODEL]\n",
      "                             [--act ACT] [--continue_train CONTINUE_TRAIN]\n",
      "                             [--pre_train PRE_TRAIN] [--extend EXTEND]\n",
      "                             [--n_resblocks N_RESBLOCKS] [--n_feats N_FEATS]\n",
      "                             [--res_scale RES_SCALE] [--shift_mean SHIFT_MEAN]\n",
      "                             [--precision {single,half,double}]\n",
      "                             [--train_name TRAIN_NAME] [--reset]\n",
      "                             [--test_every TEST_EVERY] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--split_batch SPLIT_BATCH] [--self_ensemble]\n",
      "                             [--test_only] [--gan_k GAN_K] [--lr LR]\n",
      "                             [--lr_decay LR_DECAY] [--decay_type DECAY_TYPE]\n",
      "                             [--gamma GAMMA] [--optimizer {SGD,ADAM,RMSprop}]\n",
      "                             [--momentum MOMENTUM] [--beta1 BETA1]\n",
      "                             [--beta2 BETA2] [--epsilon EPSILON]\n",
      "                             [--weight_decay WEIGHT_DECAY] [--loss LOSS]\n",
      "                             [--skip_threshold SKIP_THRESHOLD] [--save SAVE]\n",
      "                             [--load LOAD] [--resume RESUME] [--print_model]\n",
      "                             [--save_models] [--print_every PRINT_EVERY]\n",
      "                             [--save_results] [--n_resgroups N_RESGROUPS]\n",
      "                             [--reduction REDUCTION] [--testpath TESTPATH]\n",
      "                             [--testset TESTSET] [--degradation DEGRADATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Weifa\\AppData\\Roaming\\jupyter\\runtime\\kernel-5736a94e-b627-48e3-b02c-c9ee9598a866.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#pr dem chennel\n",
    "\n",
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v2_0,ACCESS_BARRA_v2_1\n",
    "import torch\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./model/save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    pre_train_path=args.continue_train\n",
    "    \n",
    "    if args.continue_train:\n",
    "        print(13113212)\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 2)\n",
    "    end_date=date(2011,12,25)\n",
    "#     end_date=date(2012,12,25) #if 929 is true we should substract 1 day    \n",
    "    sys = platform.system()\n",
    "    args.file_ACCESS_dir=\"../data/\"\n",
    "    args.file_BARRA_dir=\"../data/barra_aus/\"\n",
    "#     if sys == \"Windows\":\n",
    "#         init_date=date(1970, 1, 1)\n",
    "#         start_date=date(1990, 1, 2)\n",
    "#         end_date=date(1990,12,15) #if 929 is true we should substract 1 day   \n",
    "#         args.cpu=True\n",
    "# #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "# #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "#         args.file_DEM_dir=\"../DEM/\"\n",
    "#     else:\n",
    "#         args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "#         args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "#         # training_name=\"temp01\"\n",
    "#         args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    pre_train_path=\"./model/save/\"+args.train_name+\"/last_\"+str(args.channels)+\".pth\"\n",
    "    leading_time=217\n",
    "    args.leading_time_we_use=1\n",
    "    args.ensemble=11\n",
    "\n",
    "\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "    #     transforms.Resize(IMG_SIZE),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    data_set=0\n",
    "    if args.prprpr:\n",
    "        data_set=ACCESS_BARRA_v2_0(start_date,end_date,transform=train_transforms,args=args)\n",
    "    else:\n",
    "        data_set=ACCESS_BARRA_v2_1(start_date,end_date,transform=train_transforms,args=args)\n",
    "\n",
    "    train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  total | %5d\"%len(data_set))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  train | %5d\"%len(train_data))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  val   | %5d\"%len(val_data))\n",
    "\n",
    "    ###################################################################################set a the dataLoader\n",
    "    train_dataloders =DataLoader(train_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                num_workers=args.n_threads)\n",
    "    val_dataloders =DataLoader(val_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                              num_workers=args.n_threads)\n",
    "    ##\n",
    "    def prepare( l, volatile=False):\n",
    "        def _prepare(tensor):\n",
    "            if args.precision == 'half': tensor = tensor.half()\n",
    "            if args.precision == 'single': tensor = tensor.float()\n",
    "            return tensor.to(device)\n",
    "\n",
    "        return [_prepare(_l) for _l in l]\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint = utility.checkpoint(args)\n",
    "    net = model.Model(args, checkpoint)\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=True)\n",
    "    if not args.prprpr:\n",
    "        net=my_model.Modify_RCAN(net,args,checkpoint)\n",
    "\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=args.cpu)\n",
    "    \n",
    "    args.lr=0.00001\n",
    "#     criterion = nn.L1Loss()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "    # torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n",
    "    \n",
    "#     if args.resume==1:\n",
    "#         print(\"continue last train\")\n",
    "#         model_checkpoint = torch.load(pre_train_path,map_location=device)\n",
    "#     else:\n",
    "#         print(\"restart train\")\n",
    "#         model_checkpoint = torch.load(\"./model/save/\"+args.train_name+\"/first_\"+str(args.channels)+\".pth\",map_location=device)\n",
    "\n",
    "#     my_net.load_state_dict(model_checkpoint['model'])\n",
    "#     optimizer_my.load_state_dict(model_checkpoint['optimizer'])\n",
    "#     epoch = model_checkpoint['epoch']\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        write_log(\"!!!!!!!!!!!!!Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        net = nn.DataParallel(net,range(torch.cuda.device_count()))\n",
    "    else:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "#     my_net = torch.nn.DataParallel(my_net)\n",
    "    net.to(device)\n",
    "    \n",
    "    ##########################################################################training\n",
    "\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        scheduler.step()\n",
    "        net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,hr,_,_) in enumerate(train_dataloders):\n",
    "#             write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            start=time.time()\n",
    "            pr,hr= prepare([pr,hr])\n",
    "\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = net(pr,0)\n",
    "                running_loss =criterion(sr, hr)\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "                \n",
    "            loss+=running_loss.item() #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "                write_log(\"Train done,train time cost %f s,loss: %f\"%(start-time.time(),running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for idx_img, (lr,hr,_,_) in enumerate(val_dataloders):\n",
    "                lr,hr = prepare([lr, hr])\n",
    "                sr = net(lr,0)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    write_log(str(eval_psnr))\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    write_log(str(eval_ssim))\n",
    "\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "\n",
    "        if loss<max_error:\n",
    "            max_error=loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\"_best.pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--prprpr] [--debug] [--template TEMPLATE]\n",
      "                             [--n_threads N_THREADS] [--cpu] [--n_GPUs N_GPUS]\n",
      "                             [--seed SEED] [--pr PR] [--dem] [--psl] [--zg]\n",
      "                             [--tasmax] [--tasmin]\n",
      "                             [--leading_time_we_use LEADING_TIME_WE_USE]\n",
      "                             [--ensemble ENSEMBLE] [--channels CHANNELS]\n",
      "                             [--domain DOMAIN]\n",
      "                             [--file_ACCESS_dir FILE_ACCESS_DIR]\n",
      "                             [--file_BARRA_dir FILE_BARRA_DIR]\n",
      "                             [--file_DEM_dir FILE_DEM_DIR]\n",
      "                             [--nine2nine NINE2NINE]\n",
      "                             [--date_minus_one DATE_MINUS_ONE]\n",
      "                             [--dir_demo DIR_DEMO] [--benchmark_noise]\n",
      "                             [--n_train N_TRAIN] [--n_val N_VAL]\n",
      "                             [--offset_val OFFSET_VAL] [--ext EXT]\n",
      "                             [--scale SCALE] [--patch_size PATCH_SIZE]\n",
      "                             [--rgb_range RGB_RANGE] [--n_colors N_COLORS]\n",
      "                             [--noise NOISE] [--chop] [--model MODEL]\n",
      "                             [--act ACT] [--continue_train CONTINUE_TRAIN]\n",
      "                             [--pre_train PRE_TRAIN] [--extend EXTEND]\n",
      "                             [--n_resblocks N_RESBLOCKS] [--n_feats N_FEATS]\n",
      "                             [--res_scale RES_SCALE] [--shift_mean SHIFT_MEAN]\n",
      "                             [--precision {single,half,double}]\n",
      "                             [--train_name TRAIN_NAME] [--reset]\n",
      "                             [--test_every TEST_EVERY] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--split_batch SPLIT_BATCH] [--self_ensemble]\n",
      "                             [--test_only] [--gan_k GAN_K] [--lr LR]\n",
      "                             [--lr_decay LR_DECAY] [--decay_type DECAY_TYPE]\n",
      "                             [--gamma GAMMA] [--optimizer {SGD,ADAM,RMSprop}]\n",
      "                             [--momentum MOMENTUM] [--beta1 BETA1]\n",
      "                             [--beta2 BETA2] [--epsilon EPSILON]\n",
      "                             [--weight_decay WEIGHT_DECAY] [--loss LOSS]\n",
      "                             [--skip_threshold SKIP_THRESHOLD] [--save SAVE]\n",
      "                             [--load LOAD] [--resume RESUME] [--print_model]\n",
      "                             [--save_models] [--print_every PRINT_EVERY]\n",
      "                             [--save_results] [--n_resgroups N_RESGROUPS]\n",
      "                             [--reduction REDUCTION] [--testpath TESTPATH]\n",
      "                             [--testset TESTSET] [--degradation DEGRADATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Weifa\\AppData\\Roaming\\jupyter\\runtime\\kernel-5736a94e-b627-48e3-b02c-c9ee9598a866.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#pr dem chennel\n",
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v2_0,ACCESS_BARRA_v2_1\n",
    "import torch\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    my_log_file=open(\"./model/save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "#     pre_train_path=\"./model/save/temp01/\"+0+\".pth\"\n",
    "\n",
    "    \n",
    "    \n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 2)\n",
    "    end_date=date(2011,12,25)\n",
    "#     end_date=date(2012,12,25) #if 929 is true we should substract 1 day    \n",
    "    sys = platform.system()\n",
    "    args.file_ACCESS_dir=\"../data/\"\n",
    "    args.file_BARRA_dir=\"../data/barra_aus/\"\n",
    "#     if sys == \"Windows\":\n",
    "#         init_date=date(1970, 1, 1)\n",
    "#         start_date=date(1990, 1, 2)\n",
    "#         end_date=date(1990,12,15) #if 929 is true we should substract 1 day   \n",
    "#         args.cpu=True\n",
    "# #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "# #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "#         args.file_DEM_dir=\"../DEM/\"\n",
    "#     else:\n",
    "#         args.file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "#         args.file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "#         # training_name=\"temp01\"\n",
    "#         args.file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "    pre_train_path=\"./model/save/\"+args.train_name+\"/last_\"+str(args.channels)+\".pth\"\n",
    "    leading_time=217\n",
    "    args.leading_time_we_use=1\n",
    "    args.ensemble=1\n",
    "\n",
    "\n",
    "    print(access_rgb_mean)\n",
    "\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only？ | %5d\"%args.cpu)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "    #     transforms.Resize(IMG_SIZE),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "#     data_set=ACCESS_BARRA_v2_0(start_date,end_date,transform=train_transforms,args=args)\n",
    "    data_set=ACCESS_BARRA_v2_1(start_date,end_date,transform=train_transforms,args=args)\n",
    "\n",
    "    train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])\n",
    "\n",
    "\n",
    "    print(\"Dataset statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  total | %5d\"%len(data_set))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  train | %5d\"%len(train_data))\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  val   | %5d\"%len(val_data))\n",
    "\n",
    "    ###################################################################################set a the dataLoader\n",
    "    train_dataloders =DataLoader(train_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                num_workers=args.n_threads)\n",
    "    val_dataloders =DataLoader(val_data,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                              num_workers=args.n_threads)\n",
    "    ##\n",
    "    def prepare( l, volatile=False):\n",
    "        def _prepare(tensor):\n",
    "            if args.precision == 'half': tensor = tensor.half()\n",
    "            if args.precision == 'single': tensor = tensor.float()\n",
    "            return tensor.to(device)\n",
    "\n",
    "        return [_prepare(_l) for _l in l]\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    checkpoint = utility.checkpoint(args)\n",
    "    net = model.Model(args, checkpoint)\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=True)\n",
    "    net=my_model.Modify_RCAN(net,args,checkpoint)\n",
    "\n",
    "#     net.load(\"./model/RCAN_BIX4.pt\", pre_train=\"./model/RCAN_BIX4.pt\", resume=args.resume, cpu=args.cpu)\n",
    "    \n",
    "    args.lr=0.00001\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer_my, step_size=7, gamma=0.1)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "    # torch.optim.lr_scheduler.MultiStepLR(optimizer_my, milestones=[20,80], gamma=0.1)\n",
    "    \n",
    "#     if args.resume==1:\n",
    "#         print(\"continue last train\")\n",
    "#         model_checkpoint = torch.load(pre_train_path,map_location=device)\n",
    "#     else:\n",
    "#         print(\"restart train\")\n",
    "#         model_checkpoint = torch.load(\"./model/save/\"+args.train_name+\"/first_\"+str(args.channels)+\".pth\",map_location=device)\n",
    "\n",
    "#     my_net.load_state_dict(model_checkpoint['model'])\n",
    "#     optimizer_my.load_state_dict(model_checkpoint['optimizer'])\n",
    "#     epoch = model_checkpoint['epoch']\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        net = nn.DataParallel(net)\n",
    "    else:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "#     my_net = torch.nn.DataParallel(my_net)\n",
    "    net.to(device)\n",
    "    \n",
    "    ##########################################################################training\n",
    "\n",
    "    write_log(\"start\")\n",
    "    max_error=np.inf\n",
    "    for e in range(args.epochs):\n",
    "        #train\n",
    "        scheduler.step()\n",
    "        net.train()\n",
    "        loss=0\n",
    "        start=time.time()\n",
    "        for batch, (pr,dem,hr,_,_) in enumerate(train_dataloders):\n",
    "            write_log(\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time()))\n",
    "            start=time.time()\n",
    "            pr,dem,hr= prepare([pr,dem,hr])\n",
    "\n",
    "            optimizer_my.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                sr = net(pr,dem,0)\n",
    "                running_loss =criterion(sr, hr)\n",
    "                running_loss.backward()\n",
    "                optimizer_my.step()\n",
    "                \n",
    "            loss+=running_loss #.copy()?\n",
    "            if batch%10==0:\n",
    "                state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "                torch.save(state, \"./model/save/temp01/last.pth\")\n",
    "            write_log(\"Train done,train time cost %f s,loss: %f\"%(start-time.time(),running_loss.item()  ))\n",
    "            start=time.time()\n",
    "\n",
    "        #validation\n",
    "        net.eval()\n",
    "        start=time.time()\n",
    "        with torch.no_grad():\n",
    "            eval_psnr=0\n",
    "            eval_ssim=0\n",
    "#             tqdm_val = tqdm(val_dataloders, ncols=80)\n",
    "            for idx_img, (lr,dem,hr,_,_) in enumerate(val_dataloders):\n",
    "                lr,dem,hr = prepare([lr,dem,hr])\n",
    "                sr = net(lr,dem,0)\n",
    "                val_loss=criterion(sr, hr)\n",
    "                for ssr,hhr in zip(sr,hr):\n",
    "                    eval_psnr+=compare_psnr(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "                    eval_ssim+=compare_ssim(ssr[0].cpu().numpy(),hhr[0].cpu().numpy(),data_range=(hhr[0].cpu().max()-hhr[0].cpu().min()).item() )\n",
    "\n",
    "        write_log(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "                  e,\n",
    "                  time.time()-start,\n",
    "                  optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "                  loss.item()/len(train_data),\n",
    "                  val_loss\n",
    "             ))\n",
    "#         print(\"epoche: %d,time cost %f s, lr: %f, train_loss: %f,validation loss:%f \"%(\n",
    "#                   e,\n",
    "#                   time.time()-start,\n",
    "#                   optimizer_my.state_dict()['param_groups'][0]['lr'],\n",
    "#                   loss.item()/len(train_data),\n",
    "#                   val_loss\n",
    "#              ))\n",
    "\n",
    "        if running_loss<max_error:\n",
    "            max_error=running_loss\n",
    "    #         torch.save(net,train_loss\"_\"+str(e)+\".pkl\")\n",
    "            if not os.path.exists(\"./model/save/\"+args.train_name+\"/\"):\n",
    "                os.mkdir(\"./model/save/\"+args.train_name+\"/\")\n",
    "            write_log(\"saving\")\n",
    "            state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "            torch.save(state, \"./model/save/temp01/\"+str(e)+\".pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr dem chennel\n",
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v2_0,ACCESS_BARRA_v2_1\n",
    "import torch\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import model\n",
    "from model import my_model\n",
    "import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import xarray as xr\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4dfe18490e11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model/prprpr/best.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "torch.load(\"./model/prprpr/best.pth\")\n",
    "net.load_state_dict(torch.load(\"test.pth\")['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making model...\n",
      "accesss-s1 mean (0.4690)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = utility.checkpoint(args)\n",
    "net = model.Model(args, checkpoint)\n",
    "state = {'model': net.state_dict(), 'epoch': 1}\n",
    "torch.save(state, \"test.pth\")\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = torch.load(\"test.pth\")\n",
    "net.load_state_dict(model_checkpoint[\"model\"])\n",
    "# net.load_state_dict(model_checkpoint[\"model\"])\n",
    "\n",
    "# net=models.mnasnet0_75()\n",
    "\n",
    "# optimizer_my = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9)\n",
    "# for e in range(10):\n",
    "#     scheduler.step()\n",
    "#     print(optimizer_my.state_dict()['param_groups'][0])\n",
    "#     print(scheduler.state_dict())\n",
    "# state = {'model': net.state_dict(), 'optimizer': optimizer_my.state_dict(), 'epoch': e}\n",
    "# torch.save(state, \"test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = torch.load(\"test.pth\")\n",
    "net.load_state_dict(model_checkpoint)\n",
    "optimizer_my.load_state_dict(model_checkpoint['optimizer'])\n",
    "epoch = model_checkpoint['epoch']\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer_my, gamma=0.9,last_epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scheduler.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./model/save/123\"):\n",
    "    os.mkdir(\"./model/save/123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "import numpy as np\n",
    "np.repeat(np.expand_dims(dpt.read_barra_data_fc(\"../data/barra_aus/\",datetime(1990,1,2)),axis=2),3,axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_pr.py  --n_threads 0  --batch_size 2 --n_resgroups 10 --n_resblocks 20 --patch_size 192 --pre_train \"./model/RCAN_BIX4.pt\"\n",
    "python train_pr_dem.py  --n_threads 0  --batch_size 2 --n_resgroups 10 --n_resblocks 20 --patch_size 192 --train_name \"pr_dem\" --dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 train_pr.py  --n_threads 4  --batch_size 32 --n_resgroups 10 --n_resblocks 20 --patch_size 192 --pre_train \"./model/RCAN_BIX4.pt\" --prprpr --train_name \"prprpr\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
