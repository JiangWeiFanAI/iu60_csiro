{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# from args_parameter import args\n",
    "from PrepareData import ACCESS_BARRA_v4\n",
    "from torch.utils.data import Dataset,random_split\n",
    "import torch,os,torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# import model\n",
    "# from model import my_model\n",
    "# import utility\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.measure import compare_psnr,compare_mse\n",
    "import platform\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "def set_args():\n",
    "    parser = argparse.ArgumentParser(description='BARRA_R and ACCESS-S!')\n",
    "    parser.add_argument('--args_test', type=int, default=0,\n",
    "                            help='testing parameters input')\n",
    "    parser.add_argument('--debug', action='store_true',\n",
    "                        help='Enables debug mode')\n",
    "    parser.add_argument('--template', default='.',\n",
    "                        help='You can set various templates in option.py')\n",
    "\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "    parser.add_argument('--cpu', action='store_true',\n",
    "                        help='use cpu only')\n",
    "    parser.add_argument('--n_GPUs', type=int, default=1,\n",
    "                        help='number of GPUs')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='random seed')\n",
    "\n",
    "    # Data specifications\n",
    "    parser.add_argument('--pr', type=bool, \n",
    "                    default=True,\n",
    "                    help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--dem', type=bool, \n",
    "                    default=True,\n",
    "                    help='add-on dem?') \n",
    "    parser.add_argument('--psl', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on psl?') \n",
    "    parser.add_argument('--zg', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', type=bool, \n",
    "                    default=False,\n",
    "                    help='add-on tasmin?')\n",
    "\n",
    "    parser.add_argument('--leading_time_we_use', type=int, \n",
    "                    default=7,\n",
    "                    help='add-on tasmin?')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument('--ensemble', type=int, \n",
    "                    default=2,\n",
    "                    help='total ensambles is 11') \n",
    "\n",
    "\n",
    "    parser.add_argument('--channels', type=float, \n",
    "                        default=0,\n",
    "                        help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, \n",
    "                        default=[112.9, 154.25, -43.7425, -9.0],\n",
    "                        help='dataset directory')    \n",
    "\n",
    "\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, \n",
    "                        default=\"F:/climate/access-s1/pr/daily/\",\n",
    "\n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, \n",
    "                        default=\"C:/Users/JIA059/barra/\",\n",
    "                        help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_DEM_dir', type=str, \n",
    "                        default=\"../DEM/\",\n",
    "                        help='dataset directory')\n",
    "\n",
    "#     parser.add_argument('--nine2nine', type=bool, \n",
    "#                         default=True,\n",
    "#                         help='whether rainfall acculate from 9am to 9am')\n",
    "#     parser.add_argument('--date_minus_one', type=int, \n",
    "#                         default=1,\n",
    "#                         help='whether rainfall acculate from yesterday(1)/today(0) 9am to tody/tomorrow 9am')\n",
    "\n",
    "\n",
    "    parser.add_argument('--dir_demo', type=str, default='../test',\n",
    "                        help='demo image directory')\n",
    "    #     parser.add_argument('--data_train', type=str, default='BARRA_R',\n",
    "    #                         help='train dataset name')\n",
    "    #     parser.add_argument('--data_test', type=str, default='DIV2K',\n",
    "    #                         help='test dataset name')\n",
    "    parser.add_argument('--benchmark_noise', action='store_true',\n",
    "                        help='use noisy benchmark sets')\n",
    "    parser.add_argument('--n_train', type=int, default=800,\n",
    "                        help='number of training set')\n",
    "    parser.add_argument('--n_val', type=int, default=10,\n",
    "                        help='number of validation set')\n",
    "    parser.add_argument('--offset_val', type=int, default=800,\n",
    "                        help='validation index offest')\n",
    "    parser.add_argument('--ext', type=str, default='sep',\n",
    "                        help='dataset file extension')\n",
    "    parser.add_argument('--scale', default='4',\n",
    "                        help='super resolution scale')\n",
    "    parser.add_argument('--patch_size', type=int, default=96,\n",
    "                        help='output patch size')\n",
    "    #??????????????????????????????????????????????????\n",
    "    parser.add_argument('--rgb_range', type=int, default=400,\n",
    "                        help='maximum value of RGB')\n",
    "    parser.add_argument('--n_colors', type=int, default=1,\n",
    "                        help='number of color channels to use')\n",
    "    parser.add_argument('--noise', type=str, default='.',\n",
    "                        help='Gaussian noise std.')\n",
    "    parser.add_argument('--chop', action='store_true',\n",
    "                        help='enable memory-efficient forward')\n",
    "\n",
    "    # Model specifications\n",
    "    parser.add_argument('--model', default='RCAN',\n",
    "                        help='model name')\n",
    "\n",
    "    parser.add_argument('--act', type=str, default='relu',\n",
    "                        help='activation function')\n",
    "    parser.add_argument('--pre_train', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--extend', type=str, default='.',\n",
    "                        help='pre-trained model directory')\n",
    "    parser.add_argument('--n_resblocks', type=int, default=16,\n",
    "                        help='number of residual blocks')\n",
    "    parser.add_argument('--n_feats', type=int, default=64,\n",
    "                        help='number of feature maps')\n",
    "    parser.add_argument('--res_scale', type=float, default=1,\n",
    "                        help='residual scaling')\n",
    "    parser.add_argument('--shift_mean', default=True,\n",
    "                        help='subtract pixel mean from the input')\n",
    "    parser.add_argument('--precision', type=str, default='single',\n",
    "                        choices=('single', 'half'),\n",
    "                        help='FP precision for test (single | half)')\n",
    "\n",
    "    # Training specifications\n",
    "\n",
    "    parser.add_argument('--train_name', type=str, default='temp01',\n",
    "                        help='the trainning name of the set')\n",
    "    parser.add_argument('--reset', action='store_true',\n",
    "                        help='reset the training')\n",
    "    parser.add_argument('--test_every', type=int, default=1000,\n",
    "                        help='do test per every N batches')\n",
    "    parser.add_argument('--epochs', type=int, default=300,\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='input batch size for training')\n",
    "    parser.add_argument('--split_batch', type=int, default=1,\n",
    "                        help='split the batch into smaller chunks')\n",
    "    parser.add_argument('--self_ensemble', action='store_true',\n",
    "                        help='use self-ensemble method for test')\n",
    "    parser.add_argument('--test_only', action='store_true',\n",
    "                        help='set this option to test the model')\n",
    "    parser.add_argument('--gan_k', type=int, default=1,\n",
    "                        help='k value for adversarial loss')\n",
    "\n",
    "    # Optimization specifications\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--lr_decay', type=int, default=200,\n",
    "                        help='learning rate decay per N epochs')\n",
    "    parser.add_argument('--decay_type', type=str, default='step',\n",
    "                        help='learning rate decay type')\n",
    "    parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                        help='learning rate decay factor for step decay')\n",
    "    parser.add_argument('--optimizer', default='ADAM',\n",
    "                        choices=('SGD', 'ADAM', 'RMSprop'),\n",
    "                        help='optimizer to use (SGD | ADAM | RMSprop)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='SGD momentum')\n",
    "    parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                        help='ADAM beta1')\n",
    "    parser.add_argument('--beta2', type=float, default=0.999,\n",
    "                        help='ADAM beta2')\n",
    "    parser.add_argument('--epsilon', type=float, default=1e-8,\n",
    "                        help='ADAM epsilon for numerical stability')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay')\n",
    "\n",
    "    # Loss specifications\n",
    "    parser.add_argument('--loss', type=str, default='1*L1',\n",
    "                        help='loss function configuration')\n",
    "    parser.add_argument('--skip_threshold', type=float, default='1e6',\n",
    "                        help='skipping batch that has large error')\n",
    "\n",
    "    # Log specifications\n",
    "    parser.add_argument('--save', type=str, default='RCAN',\n",
    "                        help='file name to save')\n",
    "    parser.add_argument('--load', type=str, default='.',\n",
    "                        help='file name to load')\n",
    "    parser.add_argument('--resume', type=int, default=0,\n",
    "                        help='resume from specific checkpoint')\n",
    "    parser.add_argument('--print_model', action='store_true',\n",
    "                        help='print model')\n",
    "    parser.add_argument('--save_models', action='store_true',\n",
    "                        help='save all intermediate models')\n",
    "    parser.add_argument('--print_every', type=int, default=100,\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save_results', action='store_true',\n",
    "                        help='save output results')\n",
    "\n",
    "    # New options\n",
    "    parser.add_argument('--n_resgroups', type=int, default=10,\n",
    "                        help='number of residual groups')\n",
    "    parser.add_argument('--reduction', type=int, default=16,\n",
    "                        help='number of feature maps reduction')\n",
    "    parser.add_argument('--testpath', type=str, default='../test/DIV2K_val_LR_our',\n",
    "                        help='dataset directory for testing')\n",
    "    parser.add_argument('--testset', type=str, default='Set5',\n",
    "                        help='dataset name for testing')\n",
    "    parser.add_argument('--degradation', type=str, default='BI',\n",
    "                        help='degradation model: BI, BD')\n",
    "    # args = []\n",
    "    # args = parser.parse_known_args()[0]\n",
    "    # import platform \n",
    "    # sys = platform.system()\n",
    "    # if sys == \"Windows\":\n",
    "    #     args = parser.parse_args(args=[])\n",
    "    # else:\n",
    "    #     args = parser.parse_args()\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "\n",
    "    #     template.set_template(args)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    args.scale = list(map(lambda x: int(x), args.scale.split('+')))\n",
    "\n",
    "    if args.epochs == 0:\n",
    "        args.epochs = 1e8\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if vars(args)[arg] == 'True':\n",
    "            vars(args)[arg] = True\n",
    "        elif vars(args)[arg] == 'False':\n",
    "            vars(args)[arg] = False\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args=set_args()\n",
    "\n",
    "args.file_ACCESS_dir=\"../data/\"\n",
    "args.file_BARRA_dir=\"../data/barra_aus/\"\n",
    "args.file_DEM_dir=\"../DEM/\"\n",
    "\n",
    "init_date=date(1970, 1, 1)\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(1990,12,10)\n",
    "args.channels=0\n",
    "args.dem=0\n",
    "args.psl=0\n",
    "args.zg=1\n",
    "args.tasmax=1\n",
    "args.tasmin=0\n",
    "\n",
    "\n",
    "\n",
    "if args.pr:\n",
    "    args.channels+=1\n",
    "if args.zg:\n",
    "    args.channels+=1\n",
    "if args.psl:\n",
    "    args.channels+=1\n",
    "if args.tasmax:\n",
    "    args.channels+=1\n",
    "if args.tasmin:\n",
    "    args.channels+=1\n",
    "if args.dem:\n",
    "    args.channels+=1\n",
    "access_rgb_mean= 2.9067910245780248e-05*86400\n",
    "\n",
    "leading_time=217\n",
    "args.leading_time_we_use=7\n",
    "args.ensemble=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class ACCESS_BARRA_v2_0(Dataset):\n",
    "    '''\n",
    "    scale is size(hr)=size(lr)*scale\n",
    "    version_3_documention: compare with ver1, I modify:\n",
    "    1. access file is created on getitem,the file list is access_date,barra,barra_date,time_leading\n",
    "      in order to read more data like zg etc. more easier, we change access_filepath to access_date\n",
    "\n",
    "    2. in ver., norm the every inputs \n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",transform=None,shuffle=True,args=None):\n",
    "        print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "        print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.scale = args.scale[0]\n",
    "        self.regin = regin\n",
    "        self.leading_time=217\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "        if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "            print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "            print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filename_list)\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "            print(self.file_BARRA_dir)\n",
    "            print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc_get_lat_lon(self.file_BARRA_dir,date_for_BARRA)\n",
    "        self.lat=data_high[1]\n",
    "        self.lon=data_high[1]\n",
    "        self.shape=(79,94)\n",
    "        if self.args.dem:\n",
    "            data_dem=dpt.add_lat_lon( dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"))\n",
    "            self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for en in self.ensemble:\n",
    "            for date in self.dates:\n",
    "\n",
    "#                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "                access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#                 print(access_path)\n",
    "                if os.path.exists(access_path):\n",
    "                    for i in range(self.leading_time_we_use):\n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "    \n",
    "    #最后去掉第一行，然后shuffle\n",
    "        return _files\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "        \n",
    "        #read_data filemame[idx]\n",
    "        en,access_date,barra_date,time_leading=self.filename_list[idx]\n",
    "        \n",
    "\n",
    "        lr=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"pr\")\n",
    "        lr=np.expand_dims(lr,axis=2)\n",
    "        label=dpt.read_barra_data_fc(self.file_BARRA_dir,barra_date)\n",
    "\n",
    "        if self.args.zg:\n",
    "            lr_zg=np.expand_dims(dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"zg\"),axis=2)\n",
    "            lr=np.concatenate((lr,lr_zg),axis=2)\n",
    "\n",
    "        if self.args.psl:\n",
    "            lr_psl=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"psl\")\n",
    "\n",
    "        if self.args.tasmax:\n",
    "            lr_tasmax=np.expand_dims(dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmax\"),axis=2)\n",
    "            lr=np.concatenate((lr,lr_tasmax),axis=2)\n",
    "\n",
    "        if self.args.tasmin:\n",
    "            lr_tasmin=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmin\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        if self.transform:#channel 数量需要整理！！\n",
    "            return self.transform(lr),self.transform(label),torch.tensor(int(barra_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "#             if self.args.channels==6:\n",
    "#                 return self.transform(lr),self.transform(self.dem_data),self.transform(lr_psl),self.transform(lr_zg),self.transform(lr_tasmax),self.transform(lr_tasmin),self.transform(label),torch.tensor(int(barra_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "#             elif self.args.channels==3:\n",
    "#                 return self.transform(lr),self.transform(lr_zg),self.transform(lr_tasmax),self.transform(label),torch.tensor(int(barra_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "        else:\n",
    "            return lr*86400,label,torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "#         if self.transform:#channel 数量需要整理！！\n",
    "#             if self.args.channels==6:\n",
    "#                 return self.transform(lr),self.transform(self.dem_data),self.transform(lr_psl),self.transform(lr_zg),self.transform(lr_tasmax),self.transform(lr_tasmin),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "#             elif self.args.channels==2:\n",
    "#                 return self.transform(lr*86400),self.transform(self.dem_data),self.transform(label),torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "# #             e self.args.channels==2:\n",
    "#         else:\n",
    "#             return lr*86400,label,torch.tensor(int(date_for_BARRA.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> BARRA_R & ACCESS_S1 loading\n",
      "=> from 1990/01/02 to 2012/12/28\n",
      "../data/barra_aus/\n",
      "no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "from PrepareData import ACCESS_BARRA_v2_0\n",
    "\n",
    "data_set=ACCESS_BARRA_v2_0(\n",
    "    start_date=date(1990, 1, 2),\n",
    "    end_date=date(2011, 12, 28),\n",
    "    transform=train_transforms,\n",
    "    args=args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data=random_split(data_set,[int(len(data_set)*0.8),len(data_set)-int(len(data_set)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_threads=0\n",
    "train_dataloders =DataLoader(train_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=True,\n",
    "                            num_workers=args.n_threads)\n",
    "val_dataloders =DataLoader(val_data,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        shuffle=True,\n",
    "                          num_workers=args.n_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (lr,hr,_,_) in enumerate(train_dataloders):\n",
    "    print(batch)\n",
    "    print(torch.max(lr)    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=93452348572695624783562956\n",
    "b=236498726391862948612783461987346\n",
    "a+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
